#
#   Package manifest
#
#   Build modes
#     singleton - only install spec with a single compiler
#     cdep      - install spec using all defined compilers
#     mdep      - install spec using all MPI libraries and either all compilers
#                 or a single specified compiler
#
#   Last Revised:   15:48, 27 Dec 2025
#

singleton: access=core host=derecho,gust
    libfabric@1.22.0

singleton: access=core type=compiler
    gcc@12.5.0 <toolchain:gcc-125> <blas:openblas@0.3.30> <matrix:no>
    gcc@14.3.0 <toolchain:gcc-143> <blas:openblas@0.3.30>

# Not dependent on CUDA, but let's only install these on CUDA systems
singleton: access=core type=compiler gpu=cuda
    nvhpc@25.9 <toolchain:nvhpc-259>
    intel-oneapi-compilers@2025.2.1 <toolchain:intel-2521> <blas:intel-oneapi-mkl@2025>

singleton: access=core type=compiler host=derecho,gust
    cce@19.0.0 <toolchain:cce-1900> <blas:cray-libsci@25.03.0>

# Casper ROCm stack
singleton: gpu=rocm
    #aocc@5.0.0 <toolchain:aocc-500> <blas:amdblis@5.1> <lapack:amdlibflame@5.1> <access:core> <type:compiler>
    rocm-core@6.4.3

# These are technically installed in the external too, but Fortran modules get included into subsequent
# package builds and they are incompatible with host compilers (e.g., gcc), so this seems like the best
# workaround for now
singleton: gpu=rocm access=none compiler=core
    llvm-amdgpu@6.4.3
    rocm-openmp-extras@6.4.3       # Modifies llvm-amdgpu (yuck)

singleton: gpu=cuda access=core compiler=core
    cuda@12.9.0
    cudnn@9.8.0.87-12

# Building these build systems first will hopefully reduce concretization variance
singleton: access=core compiler=core
    cmake
    meson

singleton: access=view compiler=core
    autoconf
    autoconf-archive
    automake
    libtool
    openssl <preferred:no>
    nghttp2
    curl
    unzip
    libszip
    bzip2
    xz
    perl
    libpng
    libjpeg-turbo
    libtiff
    libtirpc
    jasper <preferred:no>
    ncurses
    readline
    sqlite
    texinfo
    bison
    ruby
    tcl
    flex
    squashfs
    squashfuse
    antlr
    zstd
    python <preferred:no>
    py-pip
    py-setuptools
    py-pyyaml
    openjdk
    tmux
    gdb
    tree
    libtree
    slirp4netns
    git
    go <cache:deps> <access:core>                 # Used by git-lfs, so we must install first
    git-lfs
    nano
    imagemagick
    qt <preferred:no> #<maxjobs:8>
    emacs
    #gnuplot    # BROKEN
    xxdiff
    xnedit
    parallel
    vim
    ncftp
    googletest
    ripgrep
    jq
    pandoc
    rclone
    openjpeg
    json-c
    postgresql
    mysql
    patchelf
    doxygen
    ed
    ferret
    ffmpeg
    #gimp   # BROKEN
    ncompress
    node-js
    npm

# Crashes with FPE when using ROCm (as of Dec 15)
singleton: gpu=cuda access=view compiler=core
    nvtop

singleton: access=none compiler=core
    lmod
    py-tabulate                                 # Used for Negin's usage script only
    py-pandas@2.1.4                             # Used for Negin's usage script only

singleton: access=core compiler=core
    miniconda3
    pixi
    uv
    apptainer
    pcre
    podman
    peak-memusage
    charliecloud
    ncl
    ncvis
    #linaro-forge@25.0.4
    darshan-util@3.4.7
    #texlive
    #matlab@R2024b
    #idl@9.1.0
    ecflow
    #grads                                      # gadap is broken
    #rstudio@2024.12.0
    wgrib2
    grib-util
    #met@11.1.0
    #metplus@5.1.0
    intel-oneapi-vtune@2025.6.0
    intel-oneapi-advisor@2025.3.0
    eigen
    madis
    neovim
    lcov
    ncview@2.1.9
    nccmp@1.9.1.0
    cdo@2.5.2
    gmt@6.5.0
    nco@5.3.4
    gsl@2.8
    cmor@3.8.0                                 # Doesn't build against numpy dependency. Why??
    heaptrack
    octave@9.4.0
    #pocl@3.0
    #vexcl@1.4.3 ^boost@1.81.0

singleton: host=derecho,gust
    #chapel                                     # Module generation issue due to being an external
    #libtorch

singleton: host=casper access=core compiler=core
    #vapor@3.10.1
    #lrose-core@20250105

# When using CUDA, a lower GCC version is better (within reason), but when using ROCm there is a
# minimum viable version tied to OS support (for ROCm 6.4.3, GCC 14)
singleton: host=casper access=core
    ucx <gpu:cuda> <compiler:core>
    ucx <gpu:rocm> <compiler:compat>

singleton: gpu=cuda
    intel-oneapi-mkl@2025.2.0 %oneapi@2025.2.1
    intel-oneapi-mkl@2025.2.0 %gcc@14.3.0

#singleton: gpu=rocm compiler=aocc@5.0.0
#    amdblis@5.1
#    amdlibflame@5.1

cdep: 
    ncarcompilers@1.1.0
    openblas@0.3.30 <compilers:gcc>
    fftw~mpi@3.3.10
    udunits@2.2.28
    hdf5~mpi@1.14.6
    hdf@4.2.15
    #h5z-zfp@1.1.1 %TC% ^hdf5 %CMP% ^zfp %CMP% <compilers:gcc>
    netcdf~mpi@4.9.3 %TC% ^hdf5 %CMP%
    netcdf-cxx@4.2 %TC% ^hdf5 %CMP% <compilers:gcc>
    #proj@9.7.0
    #geos@3.14.0
    # nvhpc skipped b/c of failed assertion; cce charconv header not found
    #gdal@3.11.3 %TC% ^hdf5 %CMP% ^netcdf-c %CMP% ^proj %CMP% ^geos %CMP% <exclude:nvhpc,cce>
    #superlu@7.0.0 %TC% ^metis %CMP% <exclude:cce>
    #eccodes@2.41.0 %TC% ^hdf5 %CMP%
    #ioapi@3.2 %TC% ^hdf5 %CMP% <compilers:gcc>
    mpi-serial@2.5.3
    parallelio~mpi@2.6.8 %TC% ^mpi-serial %CMP% ^hdf5 %CMP%
    esmf~mpi@8.9.0 %TC% ^hdf5 %CMP% ^mpi-serial %CMP%
    #libemos@4.5.1 <compilers:gcc>
    #kokkos@4.2.01
    #spherepack@3.2
    openmpi@5.0.9 <type:mpi> <host:casper>

cdep: host=derecho,gust
    #musica@0.10.1
    cray-mpich@8.1.32 %GC% <type:mpi>
    cray-mpich@9.0.0 %GC% <type:mpi>

mdep:
    parallel-netcdf@1.14.1
    hdf5+mpi@1.14.6
    netcdf+mpi@4.9.3 ^hdf5 %CMP%
    fftw+mpi@3.3.10
    #darshan-runtime@3.4.7
    parallelio+mpi@2.6.8 %TC% ^hdf5 %CMP%
    esmf+mpi@8.9.0 %TC% ^hdf5 %CMP%
    #parallelio~shared@2.6.6 ^hdf5 %CMP% %MPI% %c,cxx,fortran=%CMP%
    osu-micro-benchmarks
    #gptl@8.1.1 <exclude:cce>            # CCE build fails b/c linker flag -rdynamic not passed through by crayftn
    #superlu-dist@9.1.0 %TC% ^metis %CMP% ^parmetis %CMP% 
    #mpifileutils@0.12 <compilers:gcc> <mpis:openmpi>

mdep: host=derecho,gust
    parallelio1@1.10.1 %TC% ^hdf5 %CMP%
    #opencoarrays@2.10.2 <compilers:gcc>

mdep: gpu=cuda
    adios2+mpi@2.10.2 %c,cxx,fortran=%CMP% ^libcatalyst %CMP% ^openblas %gcc@12.5.0 ^py-mpi4py %CMP% <exclude:nvhpc>

singleton: host=casper gpu=cuda compiler=gcc@14.3.0 mpi=openmpi@5.0.8%gcc@14.3.0 access=core
    paraview@5.13.3
    visit+python@3.4.1
